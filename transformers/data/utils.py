from __future__ import absolute_import, division, print_function

import os
from typing import List

from nltk import tokenize


def list_wiki_dir(root: str) -> tuple:
    """Returns all files in the directory generated by WikiExtractor.

    The directory structure should be like this:
        root
        ├── AA
        │   ├── wiki_00
        │   ├── wiki_01
        │   └── ...
        ├── AB
        │   ├── wiki_00
        │   ├── wiki_01
        │   └── ...
        └── ...
    """
    dirs = sorted(os.listdir(root))
    filenames_list = [os.listdir(os.path.join(root, d)) for d in dirs]
    return dirs, filenames_list


def parse_wiki_file(src_file: str) -> List[str]:
    """Parses and tokenizes file generated by WikiExtractor.

    Each file contain several documents in the format:
        <doc id="" revid="" url="" title="">
            ...
            </doc>
    """
    output = []
    with open(src_file, 'r') as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            if '</doc>' == line:
                output.append('')
                continue
            if '<doc' == line[:4]:
                continue
            if '[[' == line[:2]:
                continue
            sentences = tokenize.sent_tokenize(line)
            for sentence in sentences:
                output.append(sentence)
    return output

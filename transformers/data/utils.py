from __future__ import absolute_import, division, print_function

import os
from typing import Callable, List

import blingfire
import nltk


def list_wiki_dir(root: str) -> tuple:
    """Returns all files in the directory generated by WikiExtractor.

    The directory structure should be like this:
        root
        ├── AA
        │   ├── wiki_00
        │   ├── wiki_01
        │   └── ...
        ├── AB
        │   ├── wiki_00
        │   ├── wiki_01
        │   └── ...
        └── ...
    """
    dirs = sorted(os.listdir(root))
    filenames_list = [os.listdir(os.path.join(root, d)) for d in dirs]
    return dirs, filenames_list


def extract_documents(filepath: str) -> List[List[str]]:
    """Extracts documents from file generated by WikiExtractor.

    Each file contain several documents in the format:
        <doc id="" revid="" url="" title="">
            ...
            </doc>

    Returns:
        List of documents. Each document is saved in list of str. And each element of document is
        a segment which contains one or more natural sentences except the first one which is the
        title.
    """
    documents = []
    with open(filepath, 'r') as f:
        for line in f:
            line = line.strip()
            if not line:
                continue

            if line[:5] == '<doc ':
                # start of the document
                documents.append([])
                continue
            if line == '</doc>':
                # end of the document
                continue
            if line[:2] == '[[':
                continue
            documents[-1].append(line)
    return documents


def nltk_sent_tokenize(document: List[str]) -> List[str]:
    """Tokenizes document into sentences using NLTK."""
    output = []
    for segment in document:
        sentences = nltk.tokenize.sent_tokenize(segment)
        output.extend(sentences)
    return output


def blingfire_sent_tokenize(document: List[str]) -> List[str]:
    """Tokenizes document into sentences using blingfire."""
    context = ' '.join(document).strip().replace('\n', ' ')
    return blingfire.text_to_sentences(context).split('\n')


NAME_TO_SENT_TOKENIZE_FN = {
    'nltk': nltk_sent_tokenize,
    'blingfire': blingfire_sent_tokenize,
}


def get_sent_tokenize_fn(name: str) -> Callable:
    if name not in NAME_TO_SENT_TOKENIZE_FN:
        raise KeyError(
            '{}: Unknown tokenization name. Expected are {}'.format(
                name, list(NAME_TO_SENT_TOKENIZE_FN.keys())
            )
        )
    return NAME_TO_SENT_TOKENIZE_FN[name]
